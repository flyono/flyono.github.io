---
title: 高性能异步爬虫
date: 2024-2-24 17:00:00
updated: 2024-2-24 17:00:00
categories: "Python爬虫"
tags: 
  - "Python"
  - "异步"
---

# 高性能异步爬虫

## 引入
很多同学对于异步这个概念只是停留在了“听说很NB"的认知层面上，很少有人能够在项目中真正地使用异步实现高性能的相关操作。接下来，咱们就一起来学习一下，爬虫中如何使用
异步实现高性能的数据爬取操作。
## 背景
其实爬虫的本质就是client发请求批量获取server的响应数据，如果我们有多个url待爬取，
只用一个线程且采用串行的方式执行，那只能等待爬取一个结束后才能继续下一个，效率会非常低。
所以单线程下串行多个爬虫任务是比较低效的，是因为爬虫任务是明显的O密集型（阻塞）程序。那么该如何提高爬取性能呢？
查看下述的分析测试！
## 分析处理
* 实验环境
  * 由于网络请求会受到个人电脑或者服务器响应时长不同等因素，我们可以自己创建一个专门用于测试的实验环境，搭建一个flsk服务器，爬取本机自己服务器中的数据，避免其他因素的干扰，以便非常明显的观测出异步的效果。
  * ```python
    # 环境安装 pip install flask
    
    # 创建文件夹templates，然后将test.html页面/任意一个本地页面放入即可
    
    from flask import Flask, render_template
    from time import sleep
    
    # 1.实例化对象
    app = Flask(__name__)
    
    
    # 路由地址
    @app.route('/main')
    def main():
        sleep(2)
        return 'i am main'
    
    
    # 一旦启动了服务器，在浏览器中访问路由地址，在服务器端就会执行视图函数
    @app.route('/bobo')
    def index1():
        sleep(2)
        return render_template('test.html')
    
    
    @app.route('/jay')
    def index2():
        sleep(2)
        return render_template('test.html')
    
    
    @app.route('/tom')
    def index3():
        sleep(2)
        return render_template('test.html')
    
    
    if __name__ == "__main__":
        app.run()
    ```
  * ```python
    import requests
    import time
    
    start = time.time()
    
    urls = [
        'http://127.0.0.1:5000/main',
        'http://127.0.0.1:5000/bobo',
        'http://127.0.0.1:5000/tom',
        'http://127.0.0.1:5000/jay'
    ]
    
    
    def get_request(url):
        page_text = requests.get(url=url).text
        print(len(page_text))
    
    
    for url in urls:
        get_request(url)
    
    print(time.time() - start)  # 程序运行总耗时 8.01805853843689s
    ```
  * 上述性能问题如何解决?
    * a.解决同步调用方案之多线程/多进程 `main.py`
      * 好处：使用多线程（或多进程）。多线程(或多进程)的目的是让每个爬取任务都拥有独立的线程（或进程），这样任何一个爬取任务的阻塞都不会影响其他的爬取任务。
      * 弊端：开启多进程或都线程的方式，我们是无法无限制地开启多进程或多线程的：在遇到要同时响应成百上千路的连接请求，则无论多线程还是多进程都会严重占据系统资源，降低系统对外界响应效率。
    * b.解决同步调用方案之线程/进程池
      * 好处：很多程序员可能会考虑使用“线程池”或“连接池”。“线程池“旨在减少创建和销毁线程的频率，其维持一定合理数量的线程，并让空闲的线程重新承担新的执行任务。可以很好的降低系统开销。
      * 弊端：“线程池”技术也只是在一定程度上缓解了频繁调用IO接口带来的资源占用。而且，所谓”池”始终有其上限，当请求大大超过上限时，“池”构成的系统对外界的响应并不比没有池的时候效果好多少。所以使用“池”必须考虑其面临的响应规模，并根据响应规模调整“池”的大小。
      * 通常使用`multiprocessing.dummy`线程池的方案。
      * ```python
        from multiprocessing.dummy import Pool
        ```
    * 总结
      * 对应需求中可能出现的上千甚至上万次的客户端请求，“线程池”或“连接池"或许可以缓解部分压力，但是不能解决所有问题。总之，多线程模型可以方便高效的解决小规模的服务请求，但面对大规模的服务请求，多线程模型也会遇到瓶颈。
## 终极处理方案
* 上述无论哪种解决方案其实没有解决一个性能相关的问题：
  * IO阻塞，无论是多进程还是多线程，在遇到O阻塞时都会被操作系统强行剥夺走CPU
  的执行权限（使得cu执行其他操作，其他操作可能是我们程序的其他部分，也可能是
  其他的应用程序)，我们自己程序的执行效率因此就降低了下来。
  * 解决这一问题的关键在于：
    * 我们自己从自己的应用程序级别检测到IO阻塞，然后使得cu切换到我们自己程序的
      其他部分/任务执行（这里的任务指的是当前我们自己程序表示的进程或线程中的某一
      组操作/子程序)，这样可以把我们程序的1O阻塞降到最低，我们的程序处于就绪态就
      会增多，以此来迷惑操作系统，操作系统便以为我们的程序是O阻塞比较少的程序， 从而会尽可能多的分配CPU给我们，这样也就达到了提升程序执行效率的目的。
      * 通俗理解
        * 一个线程/进程可以表示一组指定行为的操作，这个操作可以由多个执行步骤组
        成，这些执行步骤有的是阻塞操作有的非阻塞操作，那么，当cpu执行当前进程
        线程的时候遇到了阻塞的执行步骤的时候，如果不对其处理，则包含当前执行步
        骤的进程/线程就会被挂起进入到阻塞状态，且交出cpu的使用权(cpu就被别人
        抢走了)。那么如果遇到阻塞的执行步骤，我们的程序可以检测出它是阻塞的，
        且可以将cpu切换到我们自己程序其他非阻塞的执行步骤时，则包含这些执行步
        骤的进程/线程就不会进入到阻塞状态，从而减少进程/线程的阻塞状态，增加就绪状态（牢牢抢占CPU）极大幅度提升程序执行的效率.
        * 因此，有了协程后，在单进程或单线程的模式下，就可以大幅度的提升程序的运行效率了
    * 在Python3.5之后新增了`asyncio`模块，可以帮我们检测IO(只能是网络IO[HTTP连接就是网络IO操作])，实现应用程序级别的切换（异步IO)
    * 接下来让我们来了解下协程的实现，从Python3.4开始，Python中加入了协程的概
      念，但这个版本的协程还是以生成器对象为基础的，在Python3.5则增加了`asyncio`,
      使得协程的实现更加方便。首先我们需要了解下面几个概念：
      * 特殊函数：
        * 在函数定义前定义一个关键字`async`，则该函数就变成了一个特殊的函数
        * 1.特殊函数被调用后，函数内部的程序语句（函数体）没有被立即执行
        * 2.特殊函数被调用后会返回一个协程对象
      * 协程：
        * 协程对象：特殊函数调用后就可以返回/创建了一个协程对象
        * 协程对象 == 特殊的函数 == 一组指定形式的操作
          * 协程对象就是一组指定形式的操作
      * 任务：
        * 任务对象就是一个高级的协程对象
      * 事件循环：
        * 事件循环对象（Event Loop）：一个容器，用来装载任务对象的，所以说，创建好一个或多个任务对象后下一步就需要将任务对象全部装载在事件循环对象中
        * 为什么？
          * 当任务对象装载在事件循环对象中后，启动事件循环对象，其内部装载的任务对象对应的相关操作就会被立即执行。
          * 任务对象的高级之处就是
            * 可以给任务对象绑定一个**回调函数**
            * 回调函数有什么作用
              * 回调函数就是回头调用的函数，因此要这么理解，当任务对象被执行结束后，会立即调用给任务对象绑定的这个回调函数！
      * wait函数：
        * 给任务列表中的每一个对象赋予一个可被挂起的权限。当cpu执行的任务对象遇到阻塞操作的时候，当前任务对象就会被挂起，则cup就可以执行其他任务对象，提高整体程序运行的效率！
        * 挂起任务对象：让当前正在被执行的任务对象交出cpu的控制权，cpu就可以被其他任务对象抢占和使用，从而可以执行其他任务对象
          * **注意：特殊函数内部，不可以出现不支持异步代码的模块，否则会中断整个异步效果**
      * await关键字：
        * 挂起发生阻塞操作的任务对象。在任务对象表示的操作中，凡是阻塞操作的前面都必须加上`await`的关键字进行修饰
      * 真正的将多任务协程作用在爬虫中
        * 需求：爬取自己服务器中的页面数据，并将其进行数据解析操作
        * aiohttp:是一个基于网络请求的模块，功能和requests相似，但是，requests是不支持异步的，而aiohttp是支持异步的模块。
          * 环境安装：`pip install aiohttp`
          * 用法：
          ```python
            """
            实际运用异步爬虫爬取本地html文件
            """
            import asyncio
            import time
          
            import requests
            from lxml import etree
            import aiohttp
          
            start = time.time()
            urls = [
                'http://127.0.0.1:5000/bobo',
                'http://127.0.0.1:5000/tom',
                'http://127.0.0.1:5000/jay'
            ]


            async def get_request(url):
                # request是不支持异步的模块
                # response = await requests.get(url=url)
                # page_text = response.text
                # return page_text
                # 创建请求对象
                async with aiohttp.ClientSession() as sess:
                    # 基于请求对象发起请求
                    # 此处的get是发起get请求，常用参数：url, headers,params,proxy
                    # post 发起post请求
                    async with await sess.get(url=url) as response:
                        page_text = await response.text()
                        # text():获取字符串形式的响应数据
                        # read():获取二进制形式的响应数据
                        return page_text


            # 用于数据解析
            def parse(t):
                # 获取任务对象请求到的页面源码数据
                page_text = t.result()
                tree = etree.HTML(page_text)
                a = tree.xpath('//a[@id="feng"]/@href')[0]
                print(a)


            tasks = []
    
            for url in urls:
                c = get_request(url)
                task = asyncio.ensure_future(c)
                task.add_done_callback(parse)
                tasks.append(task)
    
            loop = asyncio.get_event_loop()
            loop.run_until_complete(asyncio.wait(tasks))
            
            print('总耗时:', time.time() - start)
            ```
        * 使用uvloop加速（了解） 
          * uvloop基于`libuv`,`libuv`是一个使用C语言实现的高性能异步I/O库，`uvloop`用来代替`asyncio`默认事件循环，可以进一步加快异步l/O操作的速度。
          * uvloop的使用非常简单，只要在`run_until_.complete`前，调用如下方法，将asyncio的事件循环策略设置为uvloop的事件循环策略。 
            * asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())